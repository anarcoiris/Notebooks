{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Transformer Implementation from Scratch\n",
    "\n",
    "**Project**: MiNiLLM  \n",
    "**Technologies**: Python, NumPy, PyTorch, NLP  \n",
    "**Source**: [https://github.com/anarcoiris/MiNiLLM](https://github.com/anarcoiris/MiNiLLM)\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "Low-level implementation of the Transformer architecture (Attention Is All You Need) from first principles, demonstrating deep understanding of self-attention and positional encoding.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transformer Architecture Fundamentals\n",
    "\n",
    "### Core Components\n",
    "\n",
    "```\n",
    "Input Sequence\n",
    "      \u2193\n",
    "[Token Embedding] + [Positional Encoding]\n",
    "      \u2193\n",
    "[Multi-Head Self-Attention]\n",
    "      \u2193\n",
    "[Layer Normalization]\n",
    "      \u2193\n",
    "[Feed-Forward Network]\n",
    "      \u2193\n",
    "[Layer Normalization]\n",
    "      \u2193\n",
    "(Repeat N layers)\n",
    "      \u2193\n",
    "[Output Projection]\n",
    "      \u2193\n",
    "Predicted Next Token\n",
    "```\n",
    "\n",
    "### Key Parameters\n",
    "- **Vocabulary Size**: Character-level (~100 chars)\n",
    "- **Embedding Dimension**: 384\n",
    "- **Num Heads**: 6\n",
    "- **Num Layers**: 6\n",
    "- **Context Window**: 256 characters\n",
    "- **Total Parameters**: ~10M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport sys\nfrom pathlib import Path\n\n# Try to add MiNiLLM to path (repository code available for reference only)\ntry:\n    repo_path = Path('MiNiLLM').resolve()\n    if repo_path.exists():\n        sys.path.insert(0, str(repo_path))\n        print(\"\u2713 Repository code loaded\")\n    else:\n        print(\"\u2139 Note: Repository code not found. Using standalone demo implementations.\")\nexcept Exception as e:\n    print(f\"\u2139 Note: Repository import skipped - using demo code ({e})\")\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\nprint(\"\u2713 Environment configured for transformer implementation\")\nprint(\"\\n\ud83d\udcdd Execution Note:\")\nprint(\"   This notebook demonstrates transformer architecture from first principles.\")\nprint(\"   Full production code available at: https://github.com/anarcoiris/MiNiLLM\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Self-Attention Mechanism\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "Attention(Q, K, V) = softmax(QK^T / \u221ad_k) V\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **Q** (Query): \"What am I looking for?\"\n",
    "- **K** (Key): \"What do I contain?\"\n",
    "- **V** (Value): \"What do I actually say?\"\n",
    "- **d_k**: Dimension scaling factor\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "- Run 6 attention heads in parallel\n",
    "- Each head learns different relationships\n",
    "- Concatenate and project outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simplified self-attention implementation\ndef scaled_dot_product_attention(Q, K, V, mask=None):\n    \"\"\"\n    Implement scaled dot-product attention mechanism.\n    \n    This is the core operation in transformer models, allowing the model\n    to weigh the importance of different positions in the sequence.\n    \n    Args:\n        Q: Queries (batch, seq_len, d_k) - \"What am I looking for?\"\n        K: Keys (batch, seq_len, d_k) - \"What do I contain?\"\n        V: Values (batch, seq_len, d_v) - \"What information do I provide?\"\n        mask: Causal mask for autoregressive modeling (prevents looking ahead)\n    \n    Returns:\n        output: Attention-weighted values\n        attention_weights: Attention distribution (useful for visualization)\n    \"\"\"\n    d_k = Q.shape[-1]\n    \n    # Step 1: Compute attention scores (similarity between queries and keys)\n    # Shape: (batch, seq_len, seq_len)\n    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n    # Division by sqrt(d_k) prevents scores from growing too large (gradient stability)\n    \n    # Step 2: Apply causal mask (prevent looking at future tokens)\n    if mask is not None:\n        scores = scores + mask  # mask contains -inf for future positions\n    \n    # Step 3: Softmax to get attention weights (sum to 1 across keys)\n    attention_weights = softmax(scores, axis=-1)\n    \n    # Step 4: Apply attention to values (weighted sum)\n    output = np.matmul(attention_weights, V)\n    \n    return output, attention_weights\n\ndef softmax(x, axis=-1):\n    \"\"\"\n    Numerically stable softmax implementation.\n    \n    Subtracts max value before exp() to prevent overflow.\n    \"\"\"\n    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n\n# Demo: Attention for simple sequence\nnp.random.seed(42)\nbatch, seq_len, d_model = 1, 5, 8\n\nQ = np.random.randn(batch, seq_len, d_model)\nK = np.random.randn(batch, seq_len, d_model)\nV = np.random.randn(batch, seq_len, d_model)\n\n# Create causal mask (upper triangle = -infinity)\n# This ensures token i can only attend to tokens 0...i (not future tokens)\nmask = np.triu(np.ones((seq_len, seq_len)) * -1e9, k=1)\nmask = mask[np.newaxis, :, :]  # Add batch dimension\n\noutput, weights = scaled_dot_product_attention(Q, K, V, mask)\n\nprint(\"Self-Attention Demo:\")\nprint(f\"  Input shape: {Q.shape}\")\nprint(f\"  Output shape: {output.shape}\")\nprint(f\"  Attention weights shape: {weights.shape}\")\nprint(f\"\\nAttention weights (each row sums to 1.0):\")\nprint(weights[0].round(3))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention pattern\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(weights[0], annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "            xticklabels=[f'Pos {i}' for i in range(seq_len)],\n",
    "            yticklabels=[f'Pos {i}' for i in range(seq_len)],\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "ax.set_title('Causal Self-Attention Pattern', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Key Position')\n",
    "ax.set_ylabel('Query Position')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Upper triangle is zero (causal mask prevents looking ahead)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Positional Encoding\n",
    "\n",
    "### Sinusoidal Position Embeddings\n",
    "\n",
    "**Why Needed**: Transformers have no inherent notion of sequence order\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "```\n",
    "\n",
    "**Properties**:\n",
    "- Unique encoding for each position\n",
    "- Deterministic (no learned parameters)\n",
    "- Can extrapolate to longer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encoding implementation\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Generate sinusoidal positional encodings.\n",
    "    \"\"\"\n",
    "    position = np.arange(seq_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    \n",
    "    pos_encoding = np.zeros((seq_len, d_model))\n",
    "    pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
    "    pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
    "    \n",
    "    return pos_encoding\n",
    "\n",
    "# Generate positional encodings\n",
    "seq_len, d_model = 100, 64\n",
    "pos_enc = get_positional_encoding(seq_len, d_model)\n",
    "\n",
    "print(f\"Positional Encoding:\")\n",
    "print(f\"  Shape: {pos_enc.shape}\")\n",
    "print(f\"  Range: [{pos_enc.min():.3f}, {pos_enc.max():.3f}]\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap of positional encodings\n",
    "im = axes[0].imshow(pos_enc.T, cmap='RdBu', aspect='auto', interpolation='nearest')\n",
    "axes[0].set_title('Positional Encoding Matrix', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Position')\n",
    "axes[0].set_ylabel('Embedding Dimension')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Individual position encodings\n",
    "for pos in [0, 25, 50, 75]:\n",
    "    axes[1].plot(pos_enc[pos], label=f'Position {pos}', alpha=0.7)\n",
    "axes[1].set_title('Encoding Patterns for Different Positions', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Dimension Index')\n",
    "axes[1].set_ylabel('Encoding Value')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Pipeline\n",
    "\n",
    "### Character-Level Language Modeling\n",
    "\n",
    "**Dataset Preparation**:\n",
    "1. Tokenize text into characters\n",
    "2. Create fixed-length sequences (context window)\n",
    "3. Predict next character at each position\n",
    "\n",
    "**Training Objective**:\n",
    "- Cross-entropy loss on next-token prediction\n",
    "- Minimize perplexity: exp(loss)\n",
    "\n",
    "**Optimization**:\n",
    "- AdamW optimizer\n",
    "- Learning rate schedule: warmup + cosine decay\n",
    "- Gradient clipping for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character-level tokenization\n",
    "sample_text = \"\"\"To be, or not to be, that is the question:\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles\"\"\"\n",
    "\n",
    "# Build vocabulary\n",
    "chars = sorted(list(set(sample_text)))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Encode text\n",
    "encoded = [char_to_idx[ch] for ch in sample_text]\n",
    "\n",
    "print(\"Character-Level Tokenization:\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")\n",
    "print(f\"  Unique characters: {chars}\")\n",
    "print(f\"  Encoded length: {len(encoded)}\")\n",
    "print(f\"\\nExample encoding:\")\n",
    "print(f\"  Text: '{sample_text[:30]}...'\")\n",
    "print(f\"  IDs:  {encoded[:30]}\")\n",
    "\n",
    "# Create training sequences\n",
    "def create_sequences(data, seq_len):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_len):\n",
    "        X.append(data[i:i+seq_len])\n",
    "        y.append(data[i+1:i+seq_len+1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_len = 32\n",
    "X_train, y_train = create_sequences(encoded, seq_len)\n",
    "\n",
    "print(f\"\\nTraining Sequences:\")\n",
    "print(f\"  X shape: {X_train.shape} (num_sequences, seq_len)\")\n",
    "print(f\"  y shape: {y_train.shape} (targets shifted by 1)\")\n",
    "print(f\"  Total training samples: {len(X_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference & Text Generation\n",
    "\n",
    "### Sampling Strategies\n",
    "\n",
    "**1. Greedy Decoding**:\n",
    "- Always select highest probability token\n",
    "- Fast but repetitive\n",
    "\n",
    "**2. Top-K Sampling**:\n",
    "- Sample from K most likely tokens\n",
    "- Balances quality and diversity\n",
    "\n",
    "**3. Nucleus (Top-P) Sampling**:\n",
    "- Sample from smallest set with cumulative probability > P\n",
    "- Adaptive vocabulary size\n",
    "\n",
    "**4. Temperature Scaling**:\n",
    "- T < 1: More confident (peaked distribution)\n",
    "- T > 1: More creative (flattened distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling strategies demo\n",
    "def sample_token(logits, temperature=1.0, top_k=0, top_p=1.0):\n",
    "    \"\"\"\n",
    "    Sample next token with various strategies.\n",
    "    \"\"\"\n",
    "    # Apply temperature\n",
    "    logits = logits / temperature\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = softmax(logits, axis=-1)\n",
    "    \n",
    "    # Top-K filtering\n",
    "    if top_k > 0:\n",
    "        indices_to_remove = probs < np.partition(probs, -top_k)[-top_k]\n",
    "        probs[indices_to_remove] = 0\n",
    "        probs = probs / probs.sum()\n",
    "    \n",
    "    # Nucleus (Top-P) filtering\n",
    "    if top_p < 1.0:\n",
    "        sorted_indices = np.argsort(probs)[::-1]\n",
    "        sorted_probs = probs[sorted_indices]\n",
    "        cumulative_probs = np.cumsum(sorted_probs)\n",
    "        \n",
    "        # Remove tokens with cumulative probability above threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].copy()\n",
    "        sorted_indices_to_remove[0] = False\n",
    "        \n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        probs[indices_to_remove] = 0\n",
    "        probs = probs / probs.sum()\n",
    "    \n",
    "    # Sample from distribution\n",
    "    return np.random.choice(len(probs), p=probs)\n",
    "\n",
    "# Demo: Sample with different strategies\n",
    "np.random.seed(42)\n",
    "fake_logits = np.random.randn(vocab_size)\n",
    "\n",
    "strategies = [\n",
    "    ('Greedy', {'temperature': 0.01, 'top_k': 0, 'top_p': 1.0}),\n",
    "    ('Top-K (k=5)', {'temperature': 1.0, 'top_k': 5, 'top_p': 1.0}),\n",
    "    ('Nucleus (p=0.9)', {'temperature': 1.0, 'top_k': 0, 'top_p': 0.9}),\n",
    "    ('High Temp (T=1.5)', {'temperature': 1.5, 'top_k': 0, 'top_p': 1.0}),\n",
    "]\n",
    "\n",
    "print(\"Sampling Strategy Comparison:\")\n",
    "print(f\"\\n{'Strategy':<20} {'Sampled Tokens (5 trials)'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, params in strategies:\n",
    "    samples = [sample_token(fake_logits.copy(), **params) for _ in range(5)]\n",
    "    sample_chars = [idx_to_char.get(s, '?') for s in samples]\n",
    "    print(f\"{name:<20} {sample_chars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architecture Summary\n",
    "\n",
    "### MiNiLLM Configuration\n",
    "\n",
    "```python\n",
    "config = {\n",
    "    'vocab_size': 100,        # Character vocabulary\n",
    "    'd_model': 384,           # Embedding dimension\n",
    "    'n_heads': 6,             # Attention heads\n",
    "    'n_layers': 6,            # Transformer blocks\n",
    "    'context_length': 256,    # Max sequence length\n",
    "    'dropout': 0.1,           # Regularization\n",
    "}\n",
    "```\n",
    "\n",
    "### Parameter Count\n",
    "\n",
    "- Token embeddings: 100 \u00d7 384 = 38.4K\n",
    "- Positional embeddings: 256 \u00d7 384 = 98.3K\n",
    "- Attention layers (6): ~6M\n",
    "- Feed-forward (6): ~3M\n",
    "- Output projection: 38.4K\n",
    "\n",
    "**Total**: ~10M parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model size analysis\n",
    "config = {\n",
    "    'vocab_size': 100,\n",
    "    'd_model': 384,\n",
    "    'n_heads': 6,\n",
    "    'n_layers': 6,\n",
    "    'context_length': 256,\n",
    "}\n",
    "\n",
    "# Calculate parameters\n",
    "token_emb = config['vocab_size'] * config['d_model']\n",
    "pos_emb = config['context_length'] * config['d_model']\n",
    "\n",
    "# Per-layer attention: 4 weight matrices (Q, K, V, O)\n",
    "attn_per_layer = 4 * (config['d_model'] ** 2)\n",
    "\n",
    "# Per-layer FFN: 2 weight matrices (up + down projection)\n",
    "ffn_per_layer = 2 * (config['d_model'] * 4 * config['d_model'])  # 4x expansion\n",
    "\n",
    "total_attn = attn_per_layer * config['n_layers']\n",
    "total_ffn = ffn_per_layer * config['n_layers']\n",
    "output_proj = config['vocab_size'] * config['d_model']\n",
    "\n",
    "total_params = token_emb + pos_emb + total_attn + total_ffn + output_proj\n",
    "\n",
    "print(\"Model Parameter Breakdown:\")\n",
    "print(f\"  Token embeddings:     {token_emb/1e6:.2f}M\")\n",
    "print(f\"  Position embeddings:  {pos_emb/1e6:.2f}M\")\n",
    "print(f\"  Attention layers:     {total_attn/1e6:.2f}M\")\n",
    "print(f\"  Feed-forward layers:  {total_ffn/1e6:.2f}M\")\n",
    "print(f\"  Output projection:    {output_proj/1e6:.2f}M\")\n",
    "print(f\"  \" + \"=\"*40)\n",
    "print(f\"  TOTAL PARAMETERS:     {total_params/1e6:.2f}M\")\n",
    "print(f\"\\nModel size (FP32):    {total_params * 4 / 1e6:.1f} MB\")\n",
    "print(f\"Model size (FP16):    {total_params * 2 / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Key Takeaways\n",
    "\n",
    "### Technical Achievements\n",
    "\n",
    "\u2705 **Self-Attention**: Scaled dot-product with causal masking  \n",
    "\u2705 **Positional Encoding**: Sinusoidal embeddings for sequence order  \n",
    "\u2705 **Transformer Architecture**: Multi-layer decoder with residual connections  \n",
    "\u2705 **Character-Level Modeling**: Flexible tokenization for any language  \n",
    "\u2705 **Sampling Strategies**: Greedy, top-k, nucleus, temperature scaling  \n",
    "\u2705 **Efficient Implementation**: ~10M parameters, optimized for training  \n",
    "\n",
    "### Skills Demonstrated\n",
    "\n",
    "**Deep Learning**:\n",
    "- Transformer architecture from scratch\n",
    "- Attention mechanisms\n",
    "- Sequence modeling\n",
    "\n",
    "**NLP**:\n",
    "- Tokenization strategies\n",
    "- Language modeling\n",
    "- Text generation\n",
    "\n",
    "**Engineering**:\n",
    "- Numerical stability (softmax, gradients)\n",
    "- Memory optimization\n",
    "- Inference efficiency\n",
    "\n",
    "---\n",
    "\n",
    "### Applications\n",
    "\n",
    "**Text Generation**:\n",
    "- Code completion\n",
    "- Creative writing\n",
    "- Dialogue systems\n",
    "\n",
    "**Understanding**:\n",
    "- Foundation for BERT, GPT, T5\n",
    "- Transferable to vision (ViT)\n",
    "- Multi-modal models\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- **Repository**: https://github.com/anarcoiris/MiNiLLM\n",
    "- **Papers**:\n",
    "  - \"Attention Is All You Need\" (Vaswani et al., 2017)\n",
    "  - \"Language Models are Unsupervised Multitask Learners\" (GPT-2)\n",
    "- **Technologies**: Python, NumPy, Character-level NLP\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook demonstrates low-level transformer implementation for deep learning expertise.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}