{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Containerized Hadoop Cluster Setup\n",
    "\n",
    "**Project**: Docker Hadoop  \n",
    "**Technologies**: Docker, Hadoop, HDFS, YARN, MapReduce  \n",
    "**Source**: [https://github.com/anarcoiris/docker-hadoop](https://github.com/anarcoiris/docker-hadoop)\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "Infrastructure-as-Code deployment of a multi-node Hadoop cluster using Docker Compose, demonstrating distributed system orchestration.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nfrom pathlib import Path\n\n# Try to add docker-hadoop to path (repository code available for reference only)\ntry:\n    repo_path = Path('docker-hadoop').resolve()\n    if repo_path.exists():\n        sys.path.insert(0, str(repo_path))\n        print(\"\u2713 Repository code loaded\")\n    else:\n        print(\"\u2139 Note: Repository code not found. Using standalone demo implementations.\")\nexcept Exception as e:\n    print(f\"\u2139 Note: Repository import skipped - using demo code ({e})\")\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nprint(\"\u2713 Hadoop architecture analysis ready\")\nprint(\"\\n\ud83d\udcdd Execution Note:\")\nprint(\"   This notebook demonstrates distributed systems architecture.\")\nprint(\"   Full production code available at: https://github.com/anarcoiris/docker-hadoop\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Docker Compose Configuration\n",
    "\n",
    "### docker-compose.yml Structure\n",
    "\n",
    "```yaml\n",
    "version: '3'\n",
    "services:\n",
    "  namenode:\n",
    "    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8\n",
    "    environment:\n",
    "      - CLUSTER_NAME=hadoop-cluster\n",
    "    ports:\n",
    "      - \"9870:9870\"  # Web UI\n",
    "      - \"9000:9000\"  # RPC\n",
    "  \n",
    "  datanode1:\n",
    "    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8\n",
    "    environment:\n",
    "      - SERVICE_PRECONDITION=\"namenode:9870\"\n",
    "  \n",
    "  resourcemanager:\n",
    "    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8\n",
    "    ports:\n",
    "      - \"8088:8088\"  # Web UI\n",
    "```\n",
    "\n",
    "### Key Configuration\n",
    "- **Volumes**: Persistent storage for HDFS data\n",
    "- **Networks**: Isolated Docker network for inter-node communication\n",
    "- **Health Checks**: Automatic restart on failure\n",
    "- **Resource Limits**: Memory and CPU constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster configuration parser (simplified)\n",
    "cluster_config = {\n",
    "    \"namenode\": {\n",
    "        \"memory\": \"2GB\",\n",
    "        \"cpu\": \"1.0\",\n",
    "        \"ports\": [9870, 9000]\n",
    "    },\n",
    "    \"datanodes\": [\n",
    "        {\"id\": 1, \"memory\": \"4GB\", \"cpu\": \"2.0\", \"storage\": \"50GB\"},\n",
    "        {\"id\": 2, \"memory\": \"4GB\", \"cpu\": \"2.0\", \"storage\": \"50GB\"},\n",
    "        {\"id\": 3, \"memory\": \"4GB\", \"cpu\": \"2.0\", \"storage\": \"50GB\"},\n",
    "    ],\n",
    "    \"resourcemanager\": {\n",
    "        \"memory\": \"2GB\",\n",
    "        \"cpu\": \"1.0\",\n",
    "        \"ports\": [8088]\n",
    "    },\n",
    "    \"replication_factor\": 3\n",
    "}\n",
    "\n",
    "print(\"Hadoop Cluster Configuration:\")\n",
    "print(f\"  NameNode: {cluster_config['namenode']['memory']} RAM\")\n",
    "print(f\"  DataNodes: {len(cluster_config['datanodes'])} nodes\")\n",
    "print(f\"  Total storage: {sum(int(dn['storage'][:-2]) for dn in cluster_config['datanodes'])}GB raw\")\n",
    "print(f\"  Replication factor: {cluster_config['replication_factor']}x\")\n",
    "effective_storage = sum(int(dn['storage'][:-2]) for dn in cluster_config['datanodes']) / cluster_config['replication_factor']\n",
    "print(f\"  Effective storage: {effective_storage:.0f}GB (after replication)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HDFS Operations\n",
    "\n",
    "### Common Commands\n",
    "\n",
    "```bash\n",
    "# Create directory\n",
    "hdfs dfs -mkdir -p /user/data\n",
    "\n",
    "# Upload file\n",
    "hdfs dfs -put local_file.csv /user/data/\n",
    "\n",
    "# List files\n",
    "hdfs dfs -ls /user/data\n",
    "\n",
    "# Check file replication\n",
    "hdfs fsck /user/data/file.csv -files -blocks -locations\n",
    "\n",
    "# Download file\n",
    "hdfs dfs -get /user/data/file.csv ./local_download.csv\n",
    "```\n",
    "\n",
    "### Block Management\n",
    "- Default block size: 128MB\n",
    "- Replication: 3 copies across nodes\n",
    "- Rack awareness: Distribute across racks for fault tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFS block distribution simulator\n",
    "def simulate_hdfs_blocks(file_size_mb, block_size_mb=128, replication=3):\n",
    "    \"\"\"\n",
    "    Simulate how HDFS distributes file blocks.\n",
    "    \"\"\"\n",
    "    num_blocks = (file_size_mb + block_size_mb - 1) // block_size_mb\n",
    "    total_storage = num_blocks * block_size_mb * replication\n",
    "    \n",
    "    return {\n",
    "        'file_size_mb': file_size_mb,\n",
    "        'num_blocks': num_blocks,\n",
    "        'block_size_mb': block_size_mb,\n",
    "        'replication_factor': replication,\n",
    "        'total_storage_mb': total_storage,\n",
    "        'storage_overhead': (total_storage / file_size_mb) - 1\n",
    "    }\n",
    "\n",
    "# Examples\n",
    "file_sizes = [100, 500, 1000, 5000]\n",
    "print(\"HDFS Block Distribution Analysis:\")\n",
    "print(f\"\\n{'File Size':<12} {'Blocks':<8} {'Total Storage':<15} {'Overhead'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for size in file_sizes:\n",
    "    result = simulate_hdfs_blocks(size)\n",
    "    print(f\"{size:>4d} MB      {result['num_blocks']:>3d}      \"\n",
    "          f\"{result['total_storage_mb']:>6d} MB       \"\n",
    "          f\"{result['storage_overhead']*100:>5.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MapReduce Job Example\n",
    "\n",
    "### Word Count (Classic Example)\n",
    "\n",
    "**Map Phase**:\n",
    "```python\n",
    "def mapper(line):\n",
    "    for word in line.split():\n",
    "        emit(word, 1)\n",
    "```\n",
    "\n",
    "**Reduce Phase**:\n",
    "```python\n",
    "def reducer(word, counts):\n",
    "    emit(word, sum(counts))\n",
    "```\n",
    "\n",
    "### Execution Flow\n",
    "1. **Input Split**: Divide data into chunks\n",
    "2. **Map**: Process each chunk independently\n",
    "3. **Shuffle**: Group by key\n",
    "4. **Reduce**: Aggregate results\n",
    "5. **Output**: Write to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MapReduce simulator (Python)\n",
    "from collections import defaultdict\n",
    "\n",
    "def map_phase(text_chunk):\n",
    "    \"\"\"Emit (word, 1) for each word\"\"\"\n",
    "    results = []\n",
    "    for word in text_chunk.split():\n",
    "        results.append((word.lower().strip('.,!?'), 1))\n",
    "    return results\n",
    "\n",
    "def reduce_phase(word, counts):\n",
    "    \"\"\"Sum all counts for a word\"\"\"\n",
    "    return (word, sum(counts))\n",
    "\n",
    "# Demo\n",
    "sample_text = \"\"\"hadoop is powerful hadoop enables distributed processing\n",
    "distributed systems scale horizontally hadoop and spark work together\"\"\"\n",
    "\n",
    "# Map\n",
    "map_output = map_phase(sample_text)\n",
    "print(\"Map Output (sample):\")\n",
    "print(map_output[:10])\n",
    "\n",
    "# Shuffle (group by key)\n",
    "shuffle_output = defaultdict(list)\n",
    "for word, count in map_output:\n",
    "    shuffle_output[word].append(count)\n",
    "\n",
    "# Reduce\n",
    "final_output = [reduce_phase(word, counts) for word, counts in shuffle_output.items()]\n",
    "final_output_sorted = sorted(final_output, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nWord Count Results:\")\n",
    "for word, count in final_output_sorted:\n",
    "    print(f\"  {word:<15} {count:>2d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance & Scalability\n",
    "\n",
    "### Scaling Strategy\n",
    "- **Horizontal Scaling**: Add DataNodes for storage\n",
    "- **Resource Management**: YARN allocates CPU/memory\n",
    "- **Data Locality**: Process data where it's stored\n",
    "\n",
    "### Monitoring\n",
    "- **NameNode Web UI**: http://localhost:9870\n",
    "- **ResourceManager UI**: http://localhost:8088\n",
    "- **Metrics**: Block distribution, job history, node health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance estimation\n",
    "def estimate_processing_time(data_size_gb, num_nodes, throughput_mb_per_sec=100):\n",
    "    \"\"\"\n",
    "    Estimate MapReduce job time based on cluster size.\n",
    "    \"\"\"\n",
    "    data_size_mb = data_size_gb * 1024\n",
    "    parallel_throughput = throughput_mb_per_sec * num_nodes\n",
    "    time_seconds = data_size_mb / parallel_throughput\n",
    "    return time_seconds\n",
    "\n",
    "# Scaling analysis\n",
    "data_size = 100  # GB\n",
    "node_counts = [1, 3, 5, 10]\n",
    "\n",
    "print(\"Scaling Analysis (100GB dataset):\")\n",
    "print(f\"\\n{'Nodes':<8} {'Est. Time':<12} {'Speedup'}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "baseline_time = None\n",
    "for nodes in node_counts:\n",
    "    time = estimate_processing_time(data_size, nodes)\n",
    "    if baseline_time is None:\n",
    "        baseline_time = time\n",
    "        speedup = 1.0\n",
    "    else:\n",
    "        speedup = baseline_time / time\n",
    "    \n",
    "    minutes = int(time // 60)\n",
    "    seconds = int(time % 60)\n",
    "    print(f\"{nodes:<8d} {minutes}m {seconds}s      {speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Technical Achievements\n",
    "\u2705 Docker Compose orchestration for multi-node cluster  \n",
    "\u2705 HDFS distributed storage with replication  \n",
    "\u2705 YARN resource management  \n",
    "\u2705 MapReduce job execution  \n",
    "\u2705 Horizontal scalability  \n",
    "\u2705 Infrastructure-as-Code approach  \n",
    "\n",
    "### Skills\n",
    "**DevOps**: Docker, container orchestration, infrastructure automation  \n",
    "**Distributed Systems**: HDFS, YARN, fault tolerance, data locality  \n",
    "**Big Data**: MapReduce, batch processing, cluster management  \n",
    "**Architecture**: System design, scaling strategies\n",
    "\n",
    "## References\n",
    "- **Repository**: https://github.com/anarcoiris/docker-hadoop\n",
    "- **Technologies**: Docker, Hadoop, HDFS, YARN, MapReduce\n",
    "- **Integration**: Works with PySpark for advanced analytics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}